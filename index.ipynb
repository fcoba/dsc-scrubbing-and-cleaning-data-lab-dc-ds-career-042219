{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing and Cleaning Data - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous labs, you joined the data from our separate files into a single DataFrame.  In this lab, you'll scrub the data to get it ready for exploration and modeling!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Cast columns to the appropriate data types\n",
    "* Identify and deal with null values appropriately\n",
    "* Remove unnecessary columns\n",
    "* Understand how to normalize data\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "You'll find the resulting dataset from our work in the _Obtaining Data_ Lab stored within the file `walmart_data_not_cleaned.csv`.  \n",
    "\n",
    "In the cells below:\n",
    "\n",
    "* Import pandas and set the standard alias\n",
    "* Import numpy and set the standard alias\n",
    "* Import matplotlib.pyplot and set the standard alias\n",
    "* Import seaborn and set the alias `sns` (this is the standard alias for seaborn)\n",
    "* Use the ipython magic command to set all matplotlib visualizations to display inline in the notebook\n",
    "* Load the dataset stored in the .csv file into a DataFrame using pandas\n",
    "* Inspect the head of the DataFrame to ensure everything loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, load in the dataset and inspect the head to make sure everything loaded correctly\n",
    "df = pd.read_csv('Lego_data_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting our Data Cleaning\n",
    "\n",
    "To start, you'll deal with the most obvious issue: data features with the wrong data encoding.\n",
    "\n",
    "### Checking Data Types\n",
    "\n",
    "In the cell below, use the appropriate method to check the data type of each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prod_id                int64\n",
       "ages                  object\n",
       "piece_count            int64\n",
       "set_name              object\n",
       "prod_desc             object\n",
       "prod_long_desc        object\n",
       "theme_name            object\n",
       "country               object\n",
       "list_price            object\n",
       "num_reviews          float64\n",
       "play_star_rating     float64\n",
       "review_difficulty     object\n",
       "star_rating          float64\n",
       "val_star_rating      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes #to check all columns\n",
    "# df.ages.dtypes # to check a single column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, investigate some of the unique values inside of the `list_price` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$29.99', '$19.99', '$12.99', '$99.99', '$79.99']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.list_price.unique())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Data Stored as Strings\n",
    "\n",
    "A common issue to check for at this stage is numeric columns that have accidentally been encoded as strings. For example, you should notice that the `list_price` column above is currently formatted as a string and contains a proceeding '$'. Remove this and convert the remaining number to a `float` so that you can later model this value. After all, your primary task is to generate model to predict the price.\n",
    "\n",
    "> Note: While the data spans a multitude of countries, assume for now that all prices have been standardized to USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.99, 19.99, 12.99, 99.99, 79.99])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.list_price = df.list_price.map(lambda x: float(x.replace('$', ''))) \n",
    "df.list_price.unique()[:5] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting and Dealing With Null Values\n",
    "\n",
    "Next, it's time to check for null values. How to deal with the null values will be determined by the columns containing them, and how many null values exist in each.  \n",
    " \n",
    "In the cell below, get a count of how many null values exist in each column in the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prod_id                 0\n",
       "ages                    0\n",
       "piece_count             0\n",
       "set_name                0\n",
       "prod_desc             358\n",
       "prod_long_desc          0\n",
       "theme_name              0\n",
       "country                 0\n",
       "list_price              0\n",
       "num_reviews          1421\n",
       "play_star_rating     1549\n",
       "review_difficulty    1766\n",
       "star_rating          1421\n",
       "val_star_rating      1569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() \n",
    "# df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get some descriptive statistics for each of the columns. You want to see where the minimum and maximum values lie.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prod_id</th>\n",
       "      <th>piece_count</th>\n",
       "      <th>list_price</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>play_star_rating</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>val_star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.087000e+04</td>\n",
       "      <td>10870.000000</td>\n",
       "      <td>10870.000000</td>\n",
       "      <td>9449.000000</td>\n",
       "      <td>9321.000000</td>\n",
       "      <td>9449.000000</td>\n",
       "      <td>9301.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.181634e+04</td>\n",
       "      <td>503.936431</td>\n",
       "      <td>67.309137</td>\n",
       "      <td>17.813737</td>\n",
       "      <td>4.355413</td>\n",
       "      <td>4.510319</td>\n",
       "      <td>4.214439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.736390e+05</td>\n",
       "      <td>831.209318</td>\n",
       "      <td>94.669414</td>\n",
       "      <td>38.166693</td>\n",
       "      <td>0.617272</td>\n",
       "      <td>0.516463</td>\n",
       "      <td>0.670906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.300000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.272400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.112300e+04</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>21.899000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.207350e+04</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>36.587800</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.124800e+04</td>\n",
       "      <td>556.000000</td>\n",
       "      <td>73.187800</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000431e+06</td>\n",
       "      <td>7541.000000</td>\n",
       "      <td>1104.870000</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            prod_id   piece_count    list_price  num_reviews  \\\n",
       "count  1.087000e+04  10870.000000  10870.000000  9449.000000   \n",
       "mean   6.181634e+04    503.936431     67.309137    17.813737   \n",
       "std    1.736390e+05    831.209318     94.669414    38.166693   \n",
       "min    6.300000e+02      1.000000      2.272400     1.000000   \n",
       "25%    2.112300e+04     97.000000     21.899000     2.000000   \n",
       "50%    4.207350e+04    223.000000     36.587800     6.000000   \n",
       "75%    7.124800e+04    556.000000     73.187800    14.000000   \n",
       "max    2.000431e+06   7541.000000   1104.870000   367.000000   \n",
       "\n",
       "       play_star_rating  star_rating  val_star_rating  \n",
       "count       9321.000000  9449.000000      9301.000000  \n",
       "mean           4.355413     4.510319         4.214439  \n",
       "std            0.617272     0.516463         0.670906  \n",
       "min            1.000000     1.800000         1.000000  \n",
       "25%            4.000000     4.300000         4.000000  \n",
       "50%            4.500000     4.600000         4.300000  \n",
       "75%            4.800000     5.000000         4.700000  \n",
       "max            5.000000     5.000000         5.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a bit more of a understanding of each of these features you can now make an informed decision about the best strategy for dealing with the various null values. \n",
    "\n",
    "Some common strategies for filling null values include:\n",
    "* Using the mean of the feature\n",
    "* Using the median of the feature\n",
    "* Inserting a random value from a normal distribution with the mean and std of the feature\n",
    "* Binning\n",
    "\n",
    "Given that most of the features with null values concern user reviews of the lego set, it is reasonable to wonder whether there is strong correlation between these features in the first place. Before proceeding, take a minute to investigate this hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>play_star_rating</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>val_star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_reviews</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.060884</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.026664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play_star_rating</th>\n",
       "      <td>-0.060884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619246</td>\n",
       "      <td>0.484341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>star_rating</th>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.619246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.731538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_star_rating</th>\n",
       "      <td>0.026664</td>\n",
       "      <td>0.484341</td>\n",
       "      <td>0.731538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  num_reviews  play_star_rating  star_rating  val_star_rating\n",
       "num_reviews          1.000000         -0.060884     0.004541         0.026664\n",
       "play_star_rating    -0.060884          1.000000     0.619246         0.484341\n",
       "star_rating          0.004541          0.619246     1.000000         0.731538\n",
       "val_star_rating      0.026664          0.484341     0.731538         1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Investigate whether multicollinearity exists between the review features \n",
    "#(num_reviews, play_star_rating, star_rating, val_star_rating)\n",
    "df[['num_reviews', 'play_star_rating', 'star_rating', 'val_star_rating']].corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is substantial correlation between the `play_star_rating`, `star_rating` and `val_star_rating`. While this could lead to multicollinearity in your eventual regression model, it is too early to clearly determine this at this point. Remember that multicollinearity is a relationship between 3 or more variables while correlation simply investigates the relationship between two variables.\n",
    "\n",
    "Additionally, these relationships provide an alternative method for imputing missing values: since they appear to be correlated, you could use these features to help impute missing values in the others features. For example, if you are missing the star_rating for a particular row but have the val_star_rating for that same entry, it seems reasonable to assume that it is a good estimate for the missing star_rating value as they are highly correlated. That said, doing so does come with risks; indeed you would be further increasing the correlation between these features which could further provoke multicollinearity in the final model.\n",
    "\n",
    "Investigate if you could use one of the other star rating features when one is missing. How many rows have one of `play_star_rating`, `star_rating` and `val_star_rating` missing, but not all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>play_star_rating</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>val_star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8728</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8920</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8977</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9145</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9238</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9430</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9487</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9596</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9651</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9699</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9708</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9744</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9933</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10080</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10135</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10174</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10182</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10403</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10460</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10550</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10605</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10656</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10662</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10697</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       play_star_rating  star_rating  val_star_rating\n",
       "22                  NaN          5.0              NaN\n",
       "94                  NaN          5.0              NaN\n",
       "258                 NaN          5.0              NaN\n",
       "325                 NaN          5.0              NaN\n",
       "402                 NaN          4.0              NaN\n",
       "415                 NaN          5.0              NaN\n",
       "458                 5.0          5.0              NaN\n",
       "500                 NaN          5.0              NaN\n",
       "741                 NaN          5.0              NaN\n",
       "798                 NaN          5.0              NaN\n",
       "907                 NaN          5.0              NaN\n",
       "962                 NaN          5.0              NaN\n",
       "1010                NaN          4.0              NaN\n",
       "1019                NaN          5.0              NaN\n",
       "1055                5.0          5.0              NaN\n",
       "1247                NaN          5.0              NaN\n",
       "1304                NaN          5.0              NaN\n",
       "1417                NaN          5.0              NaN\n",
       "1472                NaN          5.0              NaN\n",
       "1523                NaN          4.0              NaN\n",
       "1529                NaN          5.0              NaN\n",
       "1565                5.0          5.0              NaN\n",
       "1756                NaN          5.0              NaN\n",
       "1813                NaN          5.0              NaN\n",
       "1919                NaN          5.0              NaN\n",
       "1973                NaN          5.0              NaN\n",
       "2023                NaN          4.0              NaN\n",
       "2030                NaN          5.0              NaN\n",
       "2260                NaN          5.0              NaN\n",
       "2332                NaN          5.0              NaN\n",
       "...                 ...          ...              ...\n",
       "8692                NaN          5.0              NaN\n",
       "8728                5.0          5.0              NaN\n",
       "8920                NaN          5.0              NaN\n",
       "8977                NaN          5.0              NaN\n",
       "9090                NaN          5.0              NaN\n",
       "9145                NaN          5.0              NaN\n",
       "9194                NaN          4.0              NaN\n",
       "9202                NaN          5.0              NaN\n",
       "9238                5.0          5.0              NaN\n",
       "9430                NaN          5.0              NaN\n",
       "9487                NaN          5.0              NaN\n",
       "9596                NaN          5.0              NaN\n",
       "9651                NaN          5.0              NaN\n",
       "9699                NaN          4.0              NaN\n",
       "9708                NaN          5.0              NaN\n",
       "9744                5.0          5.0              NaN\n",
       "9933                NaN          5.0              NaN\n",
       "9986                NaN          5.0              NaN\n",
       "10080               NaN          5.0              NaN\n",
       "10135               NaN          5.0              NaN\n",
       "10174               NaN          4.0              NaN\n",
       "10182               NaN          5.0              NaN\n",
       "10218               5.0          5.0              NaN\n",
       "10403               NaN          5.0              NaN\n",
       "10460               NaN          5.0              NaN\n",
       "10550               NaN          5.0              NaN\n",
       "10605               NaN          5.0              NaN\n",
       "10656               NaN          4.0              NaN\n",
       "10662               NaN          5.0              NaN\n",
       "10697               5.0          5.0              NaN\n",
       "\n",
       "[148 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = df[['play_star_rating', 'star_rating', 'val_star_rating']]\n",
    "df_2.head()\n",
    "df_2.isna().sum()\n",
    "# len(df_2)\n",
    "\n",
    "any_nas = df_2.isna().any(axis=1)\n",
    "all_nas = df_2.isna().all(axis=1)\n",
    "df_2[(any_nas) &~ (all_nas)]\n",
    "one_or_two_na\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems like when one is missing, the other two are also apt to be missing. While this has been a bit of an extended investigation, simply go ahead and fill the missing values with that features median.  \n",
    "\n",
    "Fill in the missing `review_difficulty` values with 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data\n",
    "\n",
    "Now, you'll need to convert all of our numeric columns to the same scale by **_normalizing_** our dataset.  Recall that you normalize a dataset by converting each numeric value to it's corresponding z-score for the column, which is obtained by subtracting the column's mean and then dividing by the column's standard deviation for every value. \n",
    "\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Normalize the numeric X features by subtracting the column mean and dividing by the column standard deviation. \n",
    "(Don't bother to normalize the list_price as this is the feature you will be predicting.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Your Results\n",
    "\n",
    "While you'll once again practice one-hot encoding as you would to preprocess data before fitting a model, saving such a reperesentation of the data will eat up additional disk space. After all, a categorical variable with 10 bins will be transformed to 10 seperate features when passed through `pd.get_dummies()`. As such, while the further practice is worthwhile, save your DataFrame as is for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding Categorical Columns\n",
    "\n",
    "As a final step, you'll need to deal with the categorical columns by **_one-hot encoding_** them into binary variables via the `pd.get_dummies()` method.  \n",
    "\n",
    "When doing this, you should also subset to appropriate features. If you were to simply pass the entire DataFrame to the `pd.get_dummies()` method as it stands now, then you would end up with unique features for every single product description! (Presumably the descriptions are unique.) As such, you should first subset to the numeric features that you will eventually use in a model along with categorical variables that are not unique.\n",
    "\n",
    "In the cell below, subset to the appropriate predictive features and then use the [`pd.get_dummies()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) to one-hot encode the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You've now successfully scrubbed your dataset--you're now ready for data exploration and modeling!\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, you learned gain practice with data cleaning by:\n",
    "\n",
    "* Casting columns to the appropriate data types\n",
    "* Identifying and deal with null values appropriately\n",
    "* Removing unnecessary columns\n",
    "* Checking for and deal with multicollinearity\n",
    "* Normalizing your data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
